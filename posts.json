[{"title":"Here's how this works...","id":"heres-how-this-works","date":null,"content":"<p>Right team, listen up.\nSee, I&#39;m quite a lazy blogger - it&#39;s just too much effort, and it never seems\nto be shiny enough (Just check out my old<em>blog out for proof.\nI&#39;m going to make this easy for all of us - I&#39;ll find cool stuff on the\nInternet, or even in the Real</em>World - and I&#39;ll share it here. It might be a\nsentence, it might be a couple of paragraphs. No more pages-long blog posts for\nme!</p>\n"},{"title":"This is me. Welcome to my blog :-)","id":"this-is-me-welcome-to-my-blog--","date":null,"content":"<p>[/images/assets/<a href=\"http://www.tumblr.com/photo/1280/921866219/1/\">www.tumblr.com/photo/1280/921866219/1/</a>\ntumblr_l6tx2207cr1qd78lu.jpg]\nThis is me. Welcome to my blog :-)</p>\n"},{"title":"My new look...","id":"my-new-look","date":null,"content":"<p>Do you like my revamped theme? I think it&#39;s totally cool to have this type of\ntimeline in a microblog like Tumblr - it makes things really easy to flip\nthrough... don&#39;t you want to keep pressing the arrows?\nIt&#39;s this_theme, but I tweaked the CSS a bit and added some jQuery to bind\nleft/right scrolling to the arrow keys as well as the buttons.</p>\n"},{"title":"A handy tip","id":"a-handy-tip","date":null,"content":"<p>Just FYI...\nSee 2 posts ago - I am modifying this theme to make it work a bit nicer. Part\nof this included wanting to change some images. I was trying to just link to\nthe images in CSS, but I found a much nicer way of doing it.\nIf you want to change your theme currently, you can often change a range of\ncolors. There is a technique that allows you to prompt the user to upload an\nimage file, rather than choosing a file:\nFirst, set up the parameter that the settings drop down will look for by making\na meta tag in the document head that has it&#39;s name set as &quot;image:\nYourTemplateImage&quot;, and the content set as the link to the image that should be\nused as default.\nYou can then use this image by referring to it in your CSS or HTML - for\nexample:</p>\n\n<h1>content { background: #FFF url({image:YourTemplateImage}) no-repeat top left;</h1>\n\n<p>}\nVery nice! Cheers to this<em>blog</em>post for the link.</p>\n"},{"title":"Code Monsters","id":"code-monsters","date":null,"content":"<p>There is a monster in my code. Things keep breaking. Weird things. I think the\nmonster is eating my good code. It&#39;s the only explanation I tell you!\nI think I saw it just before, here is what it looks like. [/images/posts/2010-08-10-code-monsters-1.jpg]</p>\n"},{"title":"Interesting Ideas","id":"interesting-ideas","date":null,"content":"<p>As part of my research for my last-minute essay rewrite, I happened to pick up\nan article covering an interview with David<em>Heinemeier</em>Hansson, partner at\n37signals.com and inventor of Ruby<em>on</em>Rails.\nHansson had some very interesting ideas, including the concept of reducing\nworking hours in favour of creative quality, and the value of constraints over\ncapital: &quot;Excellent decisions come out of constraints...You must make\ncompromises and decisions to cut down and release something that&#39;s simple and\neasy to use&quot;.\nHansson also had some interesting thoughts regarding the virtual workplace,\nsuggesting that employees are happier in an environment of their choice, and a\nrelaxed managerial style that reduces overhead and emphasises quality over\nquantity.\nDefinitely somebody to keep an eye on!</p>\n"},{"title":"Time Problems","id":"time-problems","date":null,"content":"<p>Fixing time problems is probably one of the most satisfying things to fix\nprogrammatically for me. It is one of those things where you may spend ages\nstabbing in the dark with different time zones, formats, etc etc. - but there\nit really is satisfying when you do get it right and what you see in your code\nis what you see on your clock.\nWeird, but true.</p>\n"},{"title":"Rubbish Domain Names","id":"rubbish-domain-names","date":null,"content":"<p>Finding appropriate domain names is a chore. There are so many rubbish parking\nsites out there that just ... shouldn&#39;t be. I dare sayjoshmcarthur.me could be\nconsidered one of them, since I could easily offload this onto Tumblr, but I\nenjoy having the unlimited capabilities a hosted site allows.\nI think that domains should be treated in a similar way to Patents when they\nare registered. Patents require the applicant to justify to an overall\nauthority (the Intellectual<em>Property</em>Office in NZ), why they should be allowed\nto own a particular patent.\nIf there was a justification scheme in place, it would not restrict the people\nwho need domains - they would simply have to specify why they need the domain.\nWhat it would do is provide a decision framework on which to base decisions on\nregistering domains to domain agents.\nWhy tie up a perfectly good address? Leave them open, and truly make it first-\nin, first-served, instead of the &#39;richest wins&#39; model that exists at the\nmoment.</p>\n"},{"title":"Isn't Wellington beautiful?!","id":"isnt-wellington-beautiful","date":null,"content":"<p>[/images/posts/2010-08-25-wellington-1.jpg] Isn&#39;t Wellington beautiful?!</p>\n"},{"title":"No reason for this. I just thought the Beehive looked.... ma...","id":"no-reason-for-this-i-just-thought-the-beehive-looked-ma","date":null,"content":"<p>[/images/posts/2010-09-14-beehive-1.jpg] No\nreason for this. I just thought the Beehive looked.... majestic this morning :\n)</p>\n"},{"title":"RVM, Ruby-Debug, and hours of frustration","id":"rvm-ruby-debug-and-hours-of-frustration","date":null,"content":"<p>Every now and then, you run into one of THOSE problems - inexplicable, and\nsomething it seems no one else has solved before you. This afternoon, I hit one\nof these. Basically, I had just installed RVM, Ruby<em>1.8.7 and Rails</em>2.3.8 -\neverything was working great - I could run a server just fine, and use the\nrunning site.\nEventually though, I found something broken. Easy enough to diagnose, I thought\n- I&#39;ll just start up a debugging server, and work out whats going on.\nWrong. Wrong, wrong, wrong.\nWhat followed was a continuation of this error:\nYou need to install ruby-debug to run the server in debugging mode.\nSo I had a Google - lots of people have run into this problem before (here and\nhere specifically) - the problem is, most of these problems came down to a\nmissing library called libreadline - for them, installing the library corrected\nall of their problems.\nIn the end, I stumbled across this_post, from a guy having issues with Ruby\nafter upgrading his Mac OS X - finally - he suggested something I hadn&#39;t\nalready tried.\nAnd, as it turns out - it worked! I have no idea why - but there we go.\nIf you are having a problem getting Ruby 1.8.7 to acknowledge ruby-debug is\nthere when running a debugging server, try this:\ngem uninstall ruby-debug gem install ruby-debug linecache --no-ri --no-rdoc\nscript/server --debugger\n... It might just work for you!</p>\n"},{"title":"Donuts with Gemma outside Parliament","id":"donuts-with-gemma-outside-parliament","date":null,"content":"<p>[/images/posts/2010-11-09-donuts-1.jpg] Donuts\nwith Gemma outside Parliament</p>\n"},{"title":"The view from our new apartment","id":"the-view-from-our-new-apartment","date":null,"content":"<p>[/images/posts/2010-11-10-apartment-1.jpg] The view from our new apartment</p>\n"},{"title":"Animal Welfare Fail?","id":"animal-welfare-fail","date":null,"content":"<p>[/images/posts/2010-11-15-animal-welfare.jpg]\nAnimal Welfare Fail?</p>\n"},{"title":"So I decided this evening to devote a couple of hours to giv...","id":"so-i-decided-this-evening-to-devote-a-couple-of-hours-to-giv","date":null,"content":"<p>[/images/assets/<a href=\"http://www.tumblr.com/photo/1280/2196843804/1/\">www.tumblr.com/photo/1280/2196843804/1/</a>\ntumblr<em>ldcttt1WLE1qd78lu.png]\nSo I decided this evening to devote a couple of hours to giving back to Spree,\nafter using the framework for a couple of work projects, and appreciating it\nfor it&#39;s flexibility and extensibility.Â \nI&#39;ve developed an extension for Spree called Import Products. The source is\nbased on a few bits of code that I have used for 2 projects now to import\nproducts into a Spree store from a comma-seperated values file (.CSV).\nIt&#39;s not perfect, but I&#39;m happy with it as my first ever full contribution back\nto the Ruby on Rails community - I hope I have done a good enough job that it\nis useful to a couple of people.\nIf you want to check out the source code, it&#39;s on github.\nIf you want to check out a description of the extension, check out the Spree\nextension</em>page.</p>\n"},{"title":"Radiant as a Service-delivery Platform","id":"radiant-as-a-service-delivery-platform","date":null,"content":"<p>Tonight I&#39;ve been playing round with Radiant - it looks really impressive as a\ncontent-management system, and I&#39;ve been thinking about ways I can change it\nround to support vendors - that is, one person or organization offering Radiant\nas a service to multiple clients - basically, it would be one big Radiant\napplication (to simplify administration and server maintenance), but would be\nscoped (for administrative users) just to that organization - nothing would be\ndifferent from them, aside from the fact that the vendor can use one instance\nof the application to create, modify and maintain many organizations&#39; Radiant\ncontent management systems.Â \nMy motivation for doing this is to try and create something I thought of a bit\nearlier today - a service to help low-level, low-budget schools (Primary to\nIntermediate) to move services and content online in a way that they can manage\nthemselves. Radiant seems to be an excellent option in this regard - supporting\ndiverse and flexible themes, and with plenty of extensions to offer additional\nfunctionality for little work. What I want, though, is a sort of dashboard for\nmyself - I want to have a nice interface to have overall control over each\ninstallation, and for me to administer the site myself if that is a client&#39;s\npreference. In addition, by using the method of scoping a &#39;site&#39; to an\norganization, some cool things are possible, like an online signup wizard that\nwould allow a client to set up a site without any initial involvement from me -\nbecause it is just records being inserted into a database, and not an entire\napplication being created, this type of process becomes very easy.\nIt&#39;s early days yet, of course, but something in the educational area is what\nI&#39;ve been wanting to tackle for a while now - let&#39;s see how it goes.\nFor more information about Radiant CMS, seeÂ <a href=\"http://radiantcms.org/\">http://radiantcms.org/</a>\nFor more information about what I am doing, follow my github repository:\nÂ <a href=\"https://github.com/joshmcarthur/radiant-for-vendor\">https://github.com/joshmcarthur/radiant-for-vendor</a></p>\n"},{"title":"Ah, the Rugby Sevens (It's a hot day).","id":"ah-the-rugby-sevens-its-a-hot-day","date":null,"content":"<p>[/images/posts/2011-02-04-ruby-sevens-1.jpg]\nAh, the Rugby Sevens (It&#39;s a hot day).\nSorry, you&#39;ll have to imagine the sound effects for yourself...</p>\n"},{"title":"Creating a Scalable Academic Signup System","id":"creating-a-scalable-academic-signup-system","date":null,"content":"<p>While working at 3Months as a developer, I am also completing a Bachelor of\nBusiness Information Systems at Victoria University. One of the things that\nrolls around twice a year and always causes students headaches is the signups\nsystemÂ - it&#39;s one of the bragging points of the School of Information\nManagement at Victoria, as it was build as a project by a group of students\nthere (Hence the name &#39;S-Cubed = SSS = SIM Signups System). In itself, it&#39;s a\ngood application - it fulfills it&#39;s brief of managing signups for tutorials and\nworkshops, and lecturers and students alike seem to have no huge issues using\nis. The problem is though, is that its performance under load is crap - it\nsimply was not build for the load that it receives when tutorial signups for 2\ncourses of 700 people each opens at 9am, and each of those 1400 people trying\nto get the exact tutorial they want are continually refereshing the page and so\nexacerbating the server load even more.</p>\n\n<p>This server load doesn&#39;t just extend to slow page load speed - since the site\nhas not been build with scalability in mind, there have been ongoing issues\nwith sessions expiring as people try and signup for a tutorial or workshop,\npeople stuck halfway through a transaction, and the server even goes down every\nnow and then.\nWhat I am proposing is an open framework that takes the best aspects of this\ntype of system, and builds upon it techniques to optimize page load speed, and\nincrease scalability. It seems to me that Universities in general are lacking a\nsystem that allows them to smoothly run signups (usually they are dependent on\nmodified forum software, paper signups, emails or proprietary in-house\nsystems), and so this isÂ definitelyÂ something that is marketable around the\nworld.\nAs I see it, here are the base requirements for such a system:\n    * LDAP and OAuth Authentication so that students do not need to &#39;register&#39;\n      to sign up for a tutorial\n    * Accurate time-based releases of tutorials and workshops that do not\n      require human intervention\n    * Automatic server load monitoring with the ability to increase allocated\n      resources if necessary (i.e. this application is an ideal candidate for\n      Rackspace Cloud or Heroku)\n    * Per-course and per-University analytics for better scheduling of tutorial\n      and workshop release times and how students are accessing the system.\n    * The ability for lecturers and course administrators to export and print\n      class lists, tutorial lists, and workshops list with all student data or\n      just a subset.\nThis project isÂ definitelyÂ going on theÂ work pileÂ for my personal projects\n- it&#39;s got a couple of interesting aspects to it which I haven&#39;t approached\nbefore (Such as LDAP from Rails), and generally seems to follow along with my\nethos of community-oriented &#39;responsible web applications&#39;.</p>\n"},{"title":"Dynamically Serving Inline Images with Rack","id":"dynamically-serving-inline-images-with-rack","date":null,"content":"<p>Following an insanely interesting presentation by Steve Souders at Webstock, I\nbegan thinking about ways to simplify (i.e. automate) certain processes to\noptimize performance of sites. Something that immediately jumped out at me from\nearly on in Steve&#39;s workshop (So early, I was able to start coding in the\nhalfway break), was that inline images were an ideal candidate for something\nthat is relatively easy to do, but could have a significant effect on overall\npage load speed.\nThe concept of inline images has been around for a while, and basically\ninvolves embedding Base64-encoded image data within the image tag like so:\n<img src=\"data:image/jpeg;base64,[data] />\nSo, what impact does this have on performance vs. the normal technique?\nWell, the normal technique involves several more requests - one per image, in\nfact. As Steve pointed out in his workshop, server response time is only a\nsmall part of the problem - the reality is that it doesn&#39;t make a huge\ndifference how fast a piece of content is processed on the server - unless you\nare doing something really wrong, of course. Where you can really hit\nperformance bottlenecks is actually in the request-response transaction -\nwaiting for data to be transmitted, received, processed, assembled, and\nrendered on screen.\nUsing inline images can improve web performance when used appropriately, but as\nwith any other performance technique, it has it&#39;s drawbacks. First of all,\nbecause the images are actually embedded into the page, caching is affected -\nbrowsers may cache images longer than static pages, and if your web server is\nnot set up correctly, dynamic pages may not be cached at all. Next, the\nrelative file size of the page is not equal to the page size without images\nplus the combined images file size. The encoding process tends to result in\nabout a 30% increase in size.\nNotice above I have said when used appropriatelyÂ - this means that, despite\nthe drawbacks above, it really can help web performance. The key to not\noverusing this technique is to carefully consider the boundaries of what images\nshould be embedded - the best candidates are small to medium sized images that\ndo not generally change much - especially if there are quite a few of them on\nthe page and the server is slow - this means that your page load speed might be\nslower, but there will also be no need to make several requests once the page\nhasÂ loaded to draw in all of these assets. Large images, on the other hand,\nshould not generally be encoded. There is a fine balance between embedding\nimages into a page to save on requests, and overloading the page with embedded\nimages and ending up with a 2 or 3 megabyte page for the client to load. Large\nimages will shove up this page size, and in addition to this, they are usually\nparticular files that you want to apply a different caching policy to anyway.\nIn order to help out and generally feel good about my contributions to web\nperformance and open source, I have actually translated what I&#39;ve written down\nhere into a piece of Rack middleware that creates inline images (Since I&#39;ve\ncome across rack-pagespeedÂ since then, this middleware is probably\nsuperseded), and embeds them into a page. This technique was nice because a) It\ngave me a chance to learn how Rack middleware works with a practical project,\nand b) because when myself or others wish to use it, all that needs to be done\nis to drop the Gem into my Gemfile, run bundle install, and register the\nmiddleware in my Application&#39;s configuration - from there on, the middleware\nwill automatically parse responses from the Rails (Or any Rack-compatible\napplication, actually), and replace the <img src=\"file.jpg\"> with encoded data.\nOverall, this technique is not perfect - it&#39;s definitely not something to be\noverused, but with careful consideration and appropriate selection of images\nit&#39;s a good stepping stone to improved web performance.</p>\n"},{"title":"OK, So I Was Wrong","id":"ok-so-i-was-wrong","date":null,"content":"<p>OK,<em>So</em>I<em>Was</em>Wrong\nSo I was just flipping through my archives of blog posts, when I found this_one\n- my first ever post to Tumblr, where I vowed to stop posting enormously long\nessays on silly ideas I have to change the world.\nOops.\nSorry everyone!</p>\n"},{"title":"Running Rake tasks with Cron (RVM)","id":"running-rake-tasks-with-cron-rvm","date":null,"content":"<p>Recently I&#39;ve had to deal with a strange problem with rake tasks being run\nusing Cron, a UNIX tool for running commands on a scheduled basis. The problem\nwas basically the server being slowly strangled of resources across a forty to\nfifty minute time period, as each time the rake task was run it would leave an\nentire bash process sitting there consuming resources.\nI had assumed that this problem was relating to the rake task exiting in a non-\nnormal fashion - an exception maybe, or just not returning something that bash\nwas requiring to say &#39;OK, I can close now&#39;. As it turned out, the solution was\neven more basic than that.\nThe server is question is a bit of a prototype, as it&#39;s using a system-wide\ninstallation of RVM to mange two very different sets of gems. To help\nintegration with Passenger, each project directory has an .rvmrc file that\nautomatically sets the correct Ruby version and Gemset when that directory is\nentered. RVM has a neat security feature however, that prompts you to view and\napprove the file before it is executed for the first time - and, you guessed\nit, this is what was tripping up Cron.\nWhat was happening, is that Cron was changing directory to the release path in\norder to execute the rake task - but not getting as far as actually executing\nthe Rake task. Instead, Cron was being presented with a security warning from\nRVM, which was very cleverly sitting there waiting for input to approve the\nfile - causing Bash to sit there... for ever.\nFortunately, I did find a solution - it is possible to turn this security\nmechanism file off, to stop it from prompting for approval of the .rvmrc file,\nby adding a configuration flag to the default rvmrc file in /etc/rvmrc - here&#39;s\nthe configuration flag:\nexport rvm<em>trust</em>rvmrcs_flag=1\n(It goes inside what should be an if block).\nFrom now on, when Cron runs the rake tasks, it won&#39;t be presented with this\nprompt - approval will be assumed. No more server resources limits exceeded\nalerts!</p>\n"},{"title":"Spree Hosted Gateway","id":"spree-hosted-gateway","date":null,"content":"<p>Spree Hosted Gateway is my second &#39;big&#39; extension - one of the ones that isn&#39;t\njust adding one or two bits of nice functionality, but actually and end-to-end\nsolution to add something that I think would be useful to a broad range of\nSpree developers (My first was spree-import-products, see my post on it here).\nSpree (for those of you unfamiliar with it), is an open-source eCommerce\nframework written in Ruby on Rails, and has recently been upgraded to Rails 3,\nwhich has enabled extensions to now be packaged as Rails engines, making them\nfar easier to create, implement and share. Out of the box, Spree is able to\nprovide much of what a basic online store requires, but something I have felt\nis missing is support for alternative payment methods that are much more\nrudimentary than those catered for by Railsdog&#39;s<em>fork</em>of_ActiveShipping.Â \nInspired by a project I was working on at 3Months, I decided to develop an\nextension that provided a new type of Payment Method (Alongside Cheque and\nCreditcard) - External (Or Hosted) Gateway - this type of payment method was\ncharacterized in the following ways:\n    * Redirection - the customer was required by the gateway to be redirected\n      to an off-site form to make payment.\n    * Tracing the payment and order: attributes required for payment (Amount,\n      etc.) were gathered by the payment gateway from POSTed form data.\n    * Communications: there was no type of complex back-and-forth between Spree\n      and the gateway provider behind the scenes - information was transmitted\n      to the gateway, and all Spree is able to do from that point is listen for\n      a postback and collect any information from that.\nThese type of gateways are (unfortunately) quite prevalent, especially around\nsmall-to-medium independent companies - they tend to be cheaper than more\nwidely-known solutions, at the expense of a more reliable and secure\ntransaction for the customer.\nSpree Hosted Gateway, as I have mentioned, hooks into Spree in the form of\nregistering itself as a Payment Method that can be configured from the admin\ninterface. After seeing all theÂ arbitraryÂ information that I was required to\nPOST to the payment gateway I was testing against, I also made it easily\nextendable, and used Spree&#39;s preferences system to allow any number of key-\nvalue pairs to be created (Which could be modified in the Spree Admin UI),\nwhich are automatically posted to the payment gateway when I customer is\nredirected (Unless the preference key is added to the exclusion list, of\ncourse!).\nI conjunction to the &#39;transmit&#39; function of the extension, I have also\nimplemented a kind of &#39;landing pad&#39; for postbacks from the Payment Gateway to\nhit. When this happens, the extension is able to detect whether the transaction\nwas successful or not, and continue processing the order. If the customer has\nmade an payment that is not sufficient to cover the cost of the order they are\nredirected back to the payment page, and if the transaction was not successful,\nan error message is presented to them.\nAll in all, I&#39;m happy with this extension - it was a great way to learn about\nhow Spree itself handles payment methods, different types of gateway and\npayment validation and handling, and I&#39;m hoping this extension will help out\nsome of the Spree developers out there working on smaller stores that don&#39;t\nneed such a large (expensive) gateway.</p>\n"},{"title":"I18n ALWAYS escapes dots in chained backends :-(","id":"i18n-always-escapes-dots-in-chained-backends--","date":null,"content":"<p>Just a quick tip I&#39;ve come across while browsing through the comments on a\nRailscast<em>I&#39;ve</em>been<em>watching. It looks like the I18n gems that get\nautomatically installed with Rails 3 have a teensy bug.\nWhen storing a translation to the backend (be it YAML, Redis, whatever), there\nis an :escape option that can be passed to enable (or prevent) the key from\ncontaining the dot character (.) (Among others, I&#39;m sure). This seems to work\nfine when dealing with a single backend, but unfortunately when you are using\nchained backends (For example I am using Redis preferentially, but falling back\non YAML when necessary), the options hash that you would pass the :escape\noption in is mistakenly initialized to an empty hash.\nIt&#39;s a simple fix - there is already a closed pull request on the issue here.\nIt looks like the latest stable version is only 0.50 though, so the fix isn&#39;t\ngoing to filter through to the actual gem just yet. For now, the best option is\nprobably to class</em>eval into that particular section, or just change the file\nyourself.</p>\n"},{"title":"Accessing controller instance variables in model (Urgh?)","id":"accessing-controller-instance-variables-in-model-urgh","date":null,"content":"<p>I can tell that this title alone will irritate a lot of developers out there.\nIt irritated me as well, until I figured out that sometimes, doing things the\n&#39;wrong&#39; way is the best/only way. But let me get on with things.\nEvery now and then, you will come across a scenario where you need to access\nsomething in the controller, from the model. This is so obviously non-MVC that\nmany will (should) immediately shy away from it, but let me suggest a scenario.\nLet&#39;s say you have users, who should be scoped to a particular subdomain. You\nwould place a default<em>scope on the User model (with a Proc, once an issue with\ndefault</em>scope is resolved), that would automatically add conditions to any\ndatabase lookup calls to restrict the found users to those belonging to the\ncurrent subdomain. The tricky bit is, of course to get this subdomain. Because\nit is a default scope, you don&#39;t actually &#39;call&#39; it - so that rules out passing\nparameters. I&#39;ve come across other suggestions as well, such as using\ncattr<em>accessor (NO! It&#39;s shared across requests), and storing a value in\nThread.current (Relying on the application using threads in the way you expect\nthem to = not safe). So far though, the best hint I have seen here, although\nextremely anti-doing what I am suggesting, would seem to work.Â \nBasically, the jist of this post was that you could dynamically add methods to\nActiveRecord::Base using an around</em>filter to access sessions, cookies, params\nand the request. Now, I agree that this might be a bit too extreme - if you\nreally need to access the entire session or cookie hash, for example, you\nprobably aredoing something wrong. Something that I think isÂ suitable though,\ngiven an appropriate situation, is to use this technique to add an &#39;accessor&#39;\nto ActiveRecord::Base that returns the value you need in your model - the\ncurrent subdomain for instance. This is still non-MVC, but is, I think, a far\nmore elegant solution that anything else I&#39;ve seen suggested.</p>\n"},{"title":"I just found this - it's a little promotional thing that the...","id":"i-just-found-this---its-a-little-promotional-thing-that-the","date":null,"content":"<p>[/images/assets/<a href=\"http://www.tumblr.com/photo/1280/4533787560/1/\">www.tumblr.com/photo/1280/4533787560/1/</a>\ntumblr_ljiap6hYwQ1qd78lu.jpg]\nI just found this - it&#39;s a little promotional thing that the (now non-existent)\nVUW (Victoria University of Wellington IT Society) did (Well, me and Eddy) to\ntry and get people to join up (It worked.) - basically, I built a little IR\npen, and using a Wiimote, we managed to make a nice little giant touchscreen.\nGranted, it needed calibrating often, and the correct body posture and hand\nposition took some finding, but it was definately a fun experiment to try out.</p>\n"},{"title":"Using setInterval to handle scroll() events","id":"using-setinterval-to-handle-scroll-events","date":null,"content":"<p>I&#39;ve just added a nice unobtrusive scroll to top feature to my blog, and learnt\nan interesting tip in the process I thought I would share, originating from one\nof the many problems Twitter has had with it&#39;s jQuery fanciness.\nThe scroll to top stuff isn&#39;t overly complicated - just detect when the user\nhas scrolled x pixels down the screen, and then show a link to go back to the\ntop (Which can be animated if you want) - for a nice tutorial on this, check\nout<em>this</em>tutorial.\nThe thing is that this technique uses the scroll event (bound to window, but\ncan be bound to basically anything with a scrollbar). The scroll event gets\nfired any time the window is scrolled - but any time the scroll amount changes\nat all. What this means is that the scroll event is fired any time scroll\nposition changes - so if you grab the scrollbar, and pull, the scroll event is\nfired every time the bar moves, not when you release the mouse.\nJohn<em>Resig</em>reports<em>that</em>Twitter<em>ran</em>into<em>some</em>problems<em>with</em>thisÂ - they had a\nfunction bound directly to the scroll event being fired every time anyone\nscrolled (at all!). Resig has also reported a nice solution though, which I&#39;ll\npass on here. It&#39;s actually quite simple. Instead of binding a complicated\nfunction to the scroll event, you simply set a flag variable to let something\nelse know that the window has been scrolled. The second piece of the solution\nis a function running via setInterval - that is, a function being called on a\nscheduled basis. The first task of this function is to check this flag - if it\nis set, it can perform any task (Such as showing a &#39;Back to Top&#39; link, and set\nthe flag variable back to false.Â \nThe end result of this solution is that you basically have a polling function,\nrather than an event-driven one. This is actually a good thing though, when it\ncomes to this type of event. Instead of having a complex function called every\ntime the window is scrolled, a function is called every quarter second or so,\nand only executes if the window has been scrolledÂ - much better!\nI personally used an object variable, rather than just a flag - this object was\npopulated with the object that was scrolled on, and I then checked if this\nobject was populated in my polling function (rather than just is true), and\ncould then use the context it provided in this function. Since then, however, I\nhave realized that in my case, this object will alwaysbe the window object - so\nI may as well have a cheaper variable assignment and just directly refer to the\nobject in my polling function.</p>\n"},{"title":"ImageMagick: Cropping then Resizing a PNG","id":"imagemagick-cropping-then-resizing-a-png","date":null,"content":"<p>I&#39;ve been working on an image processor class for work, and recently ran into\nthis issue. I thought I would post it up here as normally I need to be quite\ndesperate before I start trawling through email mirrors - hopefully somebody\ncomes across this post first.\nIf you use ImageMagick (in particular, in conjunction with MiniMagick), then\nyou may come across this issue, and there is actually a quick fix to it. The\nissue itself is as follows: my image processor retrieves an image from an\nonline source, but the image has a 1 or 2px border, and is thousands of pixels\nwide - too wide for web use. I therefore wrote this class to first crop the\nborder off the image, and then resize it.\nIf you do actually do this though, there is an important step that needs to go\nin-between the cropping and the resizing. If you won&#39;t do this, you will\nbasically get a PNG layer that is offset from the image itself - i.e. some or\nall of the Â actual image content is not visible. This happens because when the\nimage is cropped, the origin of the image changes. It needs to be reset back to\ncoordinates 0, 0 in order to not offset the layer itself when the image is\nresized.\nHere&#39;s how to do it.Â \nFor Ruby/Minimagick:\nimage.set(&quot;page&quot;, &quot;#{image[&#39;width&#39;]}x#{image[&#39;height&#39;]}+0+0&quot;)\nFor ImageMagick directly (i.e. in console):\nconvert [image sequence]: -set page [width]x[height]+0+0\nSimple! The image should save correctly.</p>\n"},{"title":"Quick: Get random record efficiently in Rails","id":"quick-get-random-record-efficiently-in-rails","date":null,"content":"<p>You could use SQL&#39;s random function (RAND() or RANDOM() depending on database\nengine) - but this isn&#39;t database agnostic, so isn&#39;t really very quick.\nInstead you can use @nzkoz&#39;s suggested method:\nWidget.first(:offset =&gt; Widget.count)\n.... the count() method is fast, and the first() method will limit it to the\nfirst result in the SQL as well.</p>\n"},{"title":"Integration tests with Devise and RSpec","id":"integration-tests-with-devise-and-rspec","date":null,"content":"<p>RSpec 2 has supported integration tests for a while now, and I&#39;ve chosen to use\nthese for a project I&#39;m working on at the moment instead of Cucumber (I don&#39;t\nfeel that I need the verbosity andÂ English-like structure Cucumber provides\ngiven that it a more complex process to write tests).\nA bit of a problem I&#39;ve come across recently is how to get a Devise user signed\nin to your application in your tests, given that integration tests don&#39;t really\ngive you any access to either the session or the controller (This rules out\nmanually setting ID&#39;s in the session, or stubbing out current<em>user. As it turns\nout, the implementation of it is pretty simple, but I did have to do a bit of\nbrowsing and piece a few bits together to work out a nice way of doing it.\nHere is the code you can use (You would normally place this within a before(:\neach) filter in your routes spec (Which is what an integration test in RSpec is\ncalled):\n[At top of spec file, after require &#39;spec</em>helper&#39;\ninclude Warden::Test::Helpers\n[In a before(:each) block]\n@user = Factory.create(:user)\n@user.confirm!\nlogin_as @user, :scope =&gt; :user\nNow, what is this doing? Well, first of all, you include some test helpers that\nWarden (Which devise back-ends onto), provides. I tried out using Devise&#39;s\nDevise::TestHelpers here, but it looks like Devise haven&#39;t really designed\ntheir helpers with integration tests in mind - they didn&#39;t really work.Â \nWithin our before(:each) block, it now gets pretty simple. We create a User,\nusing Factory Girl (This part of the implementation doesn&#39;t really matter, you\ncan use fixtures if you prefer, or even a plain old User.create.\nNext, we confirm the user. This isn&#39;t necessary if your users haven&#39;t been\nmarked as :confirmable in your User model, but obviously our new user needs to\nbe confirmed and active in order to log in.\nLast of all, we use a helper method Warden&#39;s test helpers has provided us to\nlog in the user, which sets us up for any more requests we need to make.\nHave fun speccing nicely with Devise/Warden and RSpec!</p>\n"},{"title":"Ubuntu: Quick ImageMagick Install","id":"ubuntu-quick-imagemagick-install","date":null,"content":"<p>[This is cross-posted from a tweet I posted a while back - I think it&#39;s a nice\nbit of advice, and I wanted to store it in a more persistent form]\nInstalling ImageMagick is one of the things that Rails developers need to do\nreasonably often when provisioning new servers - basically, if you&#39;re doing any\nsort of image processing in your application (including the popularPaperclip\ngem), ImageMagick is what you&#39;ll be using.\nThe problem is that there is a bit of a magical formula I have needed to use in\nthe past - if you just install ImageMagick, it will most likely not work, as it\nneeds to have support for different image formats you want to use compiled it\nin right from the get go. Previously, I have just looked up the various\nlibraries I have needed (For PNG, JPEG, etc.), and then either found the\nlibraries in the Ubuntu package repositories or built them from source.Â \nA nice quick way of doing it though, is to use an ImageMagick meta-package in\nthe Ubuntu repositories named libmagick-9-dev - it is just a collection of\npopular image format libraries, as well as a couple of additional utilities for\nImageMagick. You can install it on any Ubuntu system by running this command:\nsudo apt-get install libmagick-9-dev\nImageMagick itself still needs to be installed, of course. The best option here\nis just to build from source - packages in the repositories are horribly out of\ndate, and I have found Paperclip, RMagick and Minimagick all require a fairly\nrecent version of ImageMagick.\nBuilding from source sounds really intimidating, but it really isn&#39;t - just\nfollow these steps:\nFirst of all, download a tarball of the ImageMagick source onto your computer:\nwget <a href=\"ftp://ftp.imagemagick.org/pub/ImageMagick/ImageMagick.tar.gz\">ftp://ftp.imagemagick.org/pub/ImageMagick/ImageMagick.tar.gz</a>\nNext, extract the tarball:\ntar -xvzf ImageMagick.tar.gz\nNow configure the package - note especially the end of the output (There is a\nlot of output) - it tells you which Image libraries you have installed - any\nwith &#39;yes&#39; next to them it will happily format and convert - because you&#39;ve\ninstalled the package above, all the common formats should have a &#39;yes&#39; next to\nthem, but it&#39;s worth checking.\ncd ImageMagick-<a href=\"VERSION%20will%20be%20a%20series%20of%20numbers%20like\" title=\"6.7.0-8\">VERSION</a>\n./configure\nNow all the hard work (By you) is done - you just need to compile the packages:\nmake &amp;&amp; sudo make install\nAll done! - ImageMagick should be all installed. To check, run the command\nwhich identify, and check it returns a path to the &#39;identify&#39; command.\nNote: If you have trouble compiling, make sure Ubunutu&#39;s build tools are\ninstalled: sudo apt-get install build-essential</p>\n"},{"title":"Quick: .rvmrc","id":"quick-rvmrc","date":null,"content":"<p>This post is probably something more experienced RVM users will already know,\nbut I wanted to post this as it&#39;s definitely my discovery of the week. When\nthrowing an .rvmrc file into a project, it&#39;s a nice thing to do to write the\nscript correctly so that it will just work for other developers (As well as\ntelling you what gemset you&#39;re using when you jump into the directory). In your\n.rvmrc file, put something like this: <code>rvm use 1.9.2@gemset --create</code> ...this\nwill attempt to use that gemset (Printing out a nice message telling you it&#39;s\nusing that one as it does so), and will create it if it doesn&#39;t exist.</p>\n"},{"title":"Rails HABTM relationships on a non-standard connection","id":"rails-habtm-relationships-on-a-non-standard-connection","date":null,"content":"<p>Recently, I&#39;ve been implementing an admin interface for a system that I want to\nmake more secure than the main application. The way I&#39;ve chosen to do this is\nto run some models that relate solely to the admin application (Authentication\nand Authorization in particular), on a different database - let&#39;s call it\n&#39;login&#39;. This seems to be a reasonably common thing to do for security\npurposes, and also for things like moving data from one database to another.\nOnce again though, I&#39;ve been tempted into the potential nest of bugs that is\nhas<em>and</em>belongs<em>to</em>many - let me explain the schema first though: *\nAdministrators table - holds email, encrypted passwords and other data about\nadministrators * Roles table - holds the name of the role - used to authorizing\nan administrator when performing an action. Each of these tables uses the line\n<code>establish_connection &#39;login&#39;</code> to connect to a different database than the\nother models - this is the secure database that I want to leave purely for the\nadministrative application. So, given that I had an administrator that should\nbe given multiple roles, and obviously each role could have many\nadministrators, has<em>and</em>belongs<em>to</em>many seemed the obvious candidate - I didn&#39;t\nreally want a model just for the association, and I would just need to write a\nmigration to create the join table. So, off, I went, and here&#39;s what happened:\n<code>ERROR: relation &quot;administrators_roles&quot; does not exist</code> i.e. - the\nAdministrator table exists, and the Role table exists, but the join table just\nisn&#39;t there. The first call for me was to take a look in the database, and make\nsure that the table was there - which it was - and that migrations had\ndefinitely run correctly and the schema was correct - which they were. After\nmuch frustration, I found <a href=\"http://groups.google.com/group/%0Arubyonrails-talk/browse_thread/thread/7644d9e5f5c6e44a/%0A69c8cce4c39eb571?show_docid=69c8cce4c39eb571\">this thread</a> which described the problems I\nhad been having - and I was vaguely satisfied to see that the problem wasn&#39;t\nreally my fault. It seems that in some versions of Rails, ActiveRecord&#39;s\nhas<em>and</em>belongs<em>to</em>many<em>association class doesn&#39;t respect the database\nconnections that the models are trying to use - instead, it uses the universal\ndatabase connection to try and look up the join table - so, what was going\nwrong was that ActiveRecord was looking in the development environment&#39;s\ndatabase, when it should have been looking in the login database.\nUnfortunately, short of updating your version of ActiveRecord/Rails or patching\nthis class, it seems that there isn&#39;t really any way of avoiding this problem -\nyou have to drop back to using has</em>many :through with a Model representing your\njoin table. I can, though, at least vouch that once you have done this, it does\nwork as expected, which, in the end, is what we want. It still feels a bit\nhacky though....</p>\n"},{"title":"'Password' or 'Passphrase'","id":"password-or-passphrase","date":null,"content":"<p>So apparently pass phrases are the new &#39;secure password&#39; - kinda the step you\nget to when you finally accept that your users are going to use something like\n&#39;password&#39; for their account password. The natural step here is to reinforce a\nsecure password strategy by requiring x numbers, x special characters and a\ncertain length - but I find this really annoying when I just want to get signed\nup, and that means that other users do as well. Something I&#39;ve just been\nthinking about is the naming semantics of password field - labeling it\n&#39;password&#39; immediately prompts users to think of an actual word - if they are\ncomputer-savvy, then they might throw a symbol or number in, but most likely it\nwill still be based on an actual word. I wonder what would happen if you\nlabelled this field &#39;Passphrase&#39; though? I think it is inevitable that many\nusers will recognize the pattern of the form rather than the labelling of the\nfields and still enter their &#39;password&#39;, but just maybe there will be some\nusers who get the semantics of the label, and enter a sentence, instead of a\nword. Even though there may not be special characters in that sentence, it&#39;s\nstill just as, if not more secure from dictionary attacks - guessing one word\nis pretty easy, but it&#39;s much, much harder to guess a string of words, in the\ncorrect order - especially if one or two of those words are obfuscated with\nsome special characters or numbers. Just a thought.... but interesting\nnonetheless.</p>\n"},{"title":"The Dictionary of New Zealand Sign Language","id":"the-dictionary-of-new-zealand-sign-language","date":null,"content":"<p><a href=\"http://nzsl.vuw.ac.nz\">The Dictionary of New Zealand Sign Language</a> went live\non Friday - it&#39;s a project that I have worked on with <a href=\"http://%0Awww.danielsherson.com\">Daniel</a>, Chris and <a href=\"http://www.linkedin.com/in/%0Ajamesarobertson\">James R</a>, at 3Months. I attended a launch event at Victoria University\non the Friday, and it really was quite a humbling experience - this dictionary\nhas been the effort of a team of people for years and years - really, what\n3Months has done has just been the tip of the iceberg. It really does seem like\nthis project has been rewarding - we&#39;ve taken a comprehensive database of sign\ninformation and images (Built with what seems to be a lot of time and effort by\n<a href=\"http://dave.moskovitz.co.nz/2011/06/24/the-online-dictionary-%0Aof-new-zealand-sign-language/\">Dave Moskozitz</a>), and given it a public presence - hopefully,\nsomething that everyone can use - whether it&#39;s to learn New Zealand Sign\nLanguage, or just to learn how to finger spell names and words. I&#39;m mostly\nposting this in the hope that everyone who reads this goes to check out the\nsite, and to learn something of this language - give it a spin! <a href=\"http://nzsl.vuw.ac.nz\">http://\nnzsl.vuw.ac.nz</a></p>\n"},{"title":"Quick: Clear Gemset","id":"quick-clear-gemset","date":null,"content":"<p>If you&#39;re finding that you have to change something fairly significant in your\nbundler dependencies, it&#39;s usually a good idea to get rid of what you&#39;ve got\nloaded in an RVM gemset so that you don&#39;t end up with different versions of\ngems fighting with each other. To do this, simply run <strong><code>rvm gemset empty</code></strong> -\nit&#39;ll delete all the gems currently in your gemset, giving you a blank slate.</p>\n"},{"title":"Arbitrary Ordering in PostgreSQL when Rails + ENUM = No.","id":"arbitrary-ordering-in-postgresql-when-rails--enum--no","date":null,"content":"<p>I&#39;ve recently had to do a custom sort for a work project that has required a\nsort on something that is not naturally sortable correctly (For example,\nalphabetical or numerical sorting). While searching for a completely different\nsolution, I came across <a href=\"http://stackoverflow.com/questions/1309624/%0Asimulating-mysqls-order-by-field-in-postgresql\">this post</a> that outlined a nice technique.\nBasically, when you have a field in your Rails model with a predefined set of\npossible values, you can use a CASE statement in PostgreSQL to perform the sort\nin whichever order these values should appear. Here&#39;s a sample of how this\ncould be achieved using Rails: <code>Result.order(&quot;CASE &quot; + &quot;WHEN medal=&#39;gold&#39;\nTHEN 1 &quot; + &quot;WHEN medal=&#39;silver&#39; THEN 2 &quot; + &quot;WHEN medal=&#39;bronze&#39; THEN 3 &quot; +\n&quot;ELSE 4 &quot; + + &quot;END,name&quot;)</code> It&#39;s messy of course - how you want to format the\nSQL string is up to you, but it&#39;s a great solution when you don&#39;t have the\nnormal sorting capabilities of a ENUM datatype available to you.</p>\n"},{"title":"Achievements on Coderwall","id":"achievements-on-coderwall","date":null,"content":"<p>Well, it&#39;s taken weeks for <a href=\"http://coderwall.com\">Coderwall</a> to finally get\nit&#39;s crawler to hit my <a href=\"https://github.com/joshmcarthur\">Github Profile</a>, but\nI&#39;ve finally <a href=\"http://coderwall.com/joshmcarthur\">got more badges</a>. I&#39;ve been\nmoving some old PHP stuff of mine onto Github for people to use, so that&#39;s\nwhere I&#39;ve gotten the PHP badge from. As part of my personal campaign to master\nat least one other language apart from Ruby, I&#39;ve also been working on some\nDjango tutorials (Django is a Python web framework), so have earned another\nbadge there. A happy accident was the walrus badge - I didn&#39;t realize I had\nprojects up in so many different languages. Probably next on this list might be\nthe &#39;Forked 20&#39; achievement for my Spree extension, <a href=\"https://github.com/joshmcarthur/spree-import-products\">spree-import-products</a> - I&#39;m not so sure this\nis a good thing, though - the forks-to-pull-requests ratio is too low on this\nproject - most likely indicating that I need to work more on making the\nextension easier to use for my general audience - forks without pull requests\nmean that people are taking their own copies to change them for their needs,\nrather than to work on bugfixes and general improvements. We&#39;ll have to see.\nBut... yay, more achievements! If you&#39;ve got a Github account, I highly\nrecommend you check out Coderwall - the achievements are largely irrelevant to\nany sort of reputation, but it&#39;s a really nice way of finding interesting\nGithub members with interesting projects.</p>\n"},{"title":"Capistrano rvm-shell: command not found error","id":"capistrano-rvm-shell-command-not-found-error","date":null,"content":"<p>I&#39;ve just come across this problem, and I had to share it here - I can&#39;t find\nanywhere else on the the Internet where the solution is specifically stated -\nit&#39;s just alluded towards. If you are using RVM&#39;s Capistrano integration, you\nmay come across a CommandNotFoundError to do with rvm-shell not being under /\nusr/local/rvm/bin (Which is exactly where it should be). Upon searching the\ninternet, you will find that you have to upgrade RVM (you don&#39;t), and that when\ninstalled for a local user, RVM puts rvm-shell under ~/bin (But, you know, it&#39;s\na system-wide install). The solution is really simple - rvm-shell is under /\nusr/local/bin - use <code>set :rvm_bin_path, &quot;/usr/local/bin&quot;</code> in your deploy\nscript, and you&#39;re away. Clearly this is a bug with RVM putting things where it\nshouldn&#39;t, but that&#39;s the way of things. And, if I sound a little short, it&#39;s\nbecause I had to all but reinstall RVM and break everything before realizing\nthis.</p>\n"},{"title":"Select Anything From Everything with Select","id":"select-anything-from-everything-with-select","date":null,"content":"<p>I was recently called upon to make a horrible select input for a Ruby on Rails project - essentially, there was this model, let&#39;s call it a Snafu, and one Snafu could share an attachment to any number of models.</p>\n\n<p>This select was difficult because I couldn&#39;t just have a selection of record ID&#39;s - I would have to use both the model type <strong>as well as</strong> the model ID in order to be able to track down these records when the form was submitted. Here&#39;s how I did it:</p>\n\n<p>*<em>Step 1: Rendering the select *</em>\nFirst of all, we need to render a selection box for the &#39;New Snafu&#39; form. This selection box should be populated by a number of model collections, keyed by an identifier that specifies both the model name and the model ID. Time for a model method, and a helper!</p>\n\n<pre><code class=\"ruby\">    def options_for_select_anything(selected = nil)\n     #Make a hash of the things we want to select from\n     options = {\n      &#39;Foo&#39; =&gt; Foo.active.map(&amp;item_for_select_anything),\n      &#39;Bar&#39; =&gt; Bar.active.map(&amp;item_for_select_anything)\n     }\n     #Use a Rails helper to actually get the tags\n     grouped_options_for_select(options, selected)\n    end\n\n    def item_for_select_anything\n     # Return an array of the record name and the identifier we want to use\n     # The record name in this case is the display value, while the identifier\n     # is the data value\n     lambda { |record| [record.name, &quot;#{record.id}-#{record.class.name}&quot;] }\n    end\n</code></pre>\n\n<p>Now that we have these helpers, we have a grouped collection of options, in which each option tag within the select will look something like &#39;<code>&lt;option value=&quot;1-Foo&quot;&gt;Foo #1&lt;/option&gt;</code>&#39; - let&#39;s render our select.</p>\n\n<pre><code class=\"erb\">    &lt;% form_for @snafu do |form| %&gt;\n        &lt;p&gt;\n            &lt;%= form.label :item_identifier, &#39;Item&#39; %&gt;&lt;br /&gt;\n            &lt;%= form.select :item_identifier, options_for_select_anything %&gt;\n        &lt;/p&gt;\n        &lt;p&gt;\n            &lt;%= form.submit &#39;Create&#39; %&gt;\n        &lt;/p&gt;\n    &lt;% end %&gt;\n</code></pre>\n\n<p>This is a basic form of course - your form will have all the other attributes you need, but the key thing to notice here is that we aren&#39;t trying to load against our Polymorphic &#39;item&#39; association within the form - instead, we&#39;re just going to send through a &#39;item_identifier&#39; parameter that we can use to <em>find</em> the item in our model. Let&#39;s take a look at the sections we need.</p>\n\n<p>First of all, we need to have the Polymorphic association in our model, if you haven&#39;t already done this.</p>\n\n<pre><code class=\"ruby\">    # Snafu.rb\n\n    class Snafu &lt; ActiveRecord::Base\n        # Snip\n        belongs_to :item, :polymorphic =&gt; true\n        # Snip\n    end\n\n    # Migration\n    add_column :snafus, :item_id, :integer, :null =&gt; false\n    add_column :snafus, :item_type, :string, :null =&gt; false\n\n</code></pre>\n\n<p>Next of all, we need to add an <code>attr_writer</code> to our model - this will hold the &#39;item_identifier&#39; from our form submission until we are ready to use it.</p>\n\n<pre><code class=\"ruby\">    # Snafu.rb\n\n    attr_writer :item_identifier\n</code></pre>\n\n<p>Finally, we need to add a <code>before_validation</code> filter to associate the record identified by our item_identifier with our model instance:</p>\n\n<pre><code class=\"ruby\">    # Snafu.rb\n    class Snafu &lt; ActiveRecord::Base\n        # Snip\n        belongs_to :item, :polymorphic =&gt; true\n        before_validation :set_item\n\n        attr_writer :item_identifier\n\n        # Snip\n\n        private\n\n        def set_item\n            if self.item_identifier\n            id, model = self.item_identifier.split(&#39;-&#39;)\n            self.item = Kernel.const_get(model).send(:find, id) rescue nil\n        end\n     end\n</code></pre>\n\n<p>Let&#39;s take a look at what we&#39;ve done there - really, the meat of it is in our set<em>item method, that gets called before validations run (so that we will have a real item set before we may need to run validations on that item). First of all, set</em>item checks that we have set a item<em>identifier - if we haven&#39;t we don&#39;t want to run into Nil exceptions! Given an item</em>identifier is present, we want to split our identifier (Remember, this is in the format id-class name), into two parts. Finally, we do a little Ruby magic to get the class using Kernel.const_get, and then call <code>find</code> on it with the ID that we want. If anything goes wrong with this bit (The class not existing, for example), then we just set item to nil.</p>\n\n<p>There we have it then - it&#39;s a horrible situation, but I feel like it&#39;s a pretty good approach. The logic is where it should be (models and helpers), the views and controllers feel clean, and it&#39;s flexible to be reused pretty easily.</p>\n\n<p>As a final tweak, there&#39;s one more change you may want to make - that is setting the selected item when we return to our form. If you take a look at the helper method we defined above, you&#39;ll notice that we already support a selected option - we just need to pass this in. To do this, we want to add a method to our &#39;Snafu&#39; class that will return a string of the item id and the item name concatenated with a dash - i.e., the format that our select box in the form is expecting. Go ahead and add the method now:</p>\n\n<pre><code class=\"ruby\">    # Snafu.rb\n\n    def item_identifier\n        # Return the owner_identifier set using the attr_writer, if it exists\n        return @owner_identifier if @owner_identifier\n\n        # Otherwise, try and build it from the current item saved against the model\n        [self.item.id, self.item.class.name].join(&#39;-&#39;) if self.item.present?\n    end\n</code></pre>\n\n<p>With that method, can can just change our select input in the form to make use of this value:</p>\n\n<pre><code class=\"ruby\">    # new.html.erb\n    # Snip\n    &lt;%= form.select :item_identifier, options_for_select_anything(form.object.item_identifier) %&gt;\n</code></pre>\n\n<p>There we are! Now when we show the form, the selected item will appear, just as we wanted.</p>\n"},{"title":"Introducing Blog Broadcaster","id":"introducing-blog-broadcaster","date":"2011-09-14 19:04","content":"<p>I&#39;ve just completed a Blog Broadcaster for this blog. It had a couple of interesting technical things, and I needed to test it properly, so here&#39;s this post!</p>\n\n<p>I recently migrated this blog from <a href=\"http://tumblr.com\">Tumblr</a>, and while Tumblr was pretty awesome and easy to use, it didn&#39;t have great support for blocks of code and preformatted comment, and, like <a href=\"http://www.radiantcms.org\">Radiant CMS</a>, it stored layouts, stylesheets, and all of that kind of thing in a database somewhere - it wasn&#39;t in source control, and making changes to it was dangerous.</p>\n\n<p>One thing that I immediately missed from Tumblr, however, was it&#39;s ability to post to Facebook and Twitter automatically whenever I published a post. As I don&#39;t post that often, it&#39;s really important to me that I market my blog as much as I can - like my Github profile, the content on my blog is a reflection of my knowledge and skill as a developer, and so I want to get that in front of people as much as possible - broadcasting to social networks is a great way of achieving that.</p>\n\n<p>With my blog on Github, I needed a way to broadcast new posts to these social networks. Github has a built-in post-receive hook to post to Twitter, but I really needed something more than that. Here&#39;s my list of requirements:</p>\n\n<ul>\n<li>It should automatically post without me needing to do any special task</li>\n<li>It should be conditional - i.e. it shouldn&#39;t post <em>everytime</em> I change something</li>\n<li>It should support both Facebook and Twitter</li>\n<li>It should be able to be triggered by a commit</li>\n</ul>\n\n<p>What I ended up building was a Sinatra app whose sole purpose was to receive POST&#39;ed commit information, parse it into a Facebook and Twitter post, and broadcast it. It&#39;s hosted on Heroku, and gets triggered by a Github <a href=\"http://help.github.com/post-receive-hooks/\">Post-Receive Hooks</a>. I did run into a couple of problems along the way - the main one was sorting out being able to post Facebook updates without being logged in, or even needing to be involved at the process at all. This wasn&#39;t too difficult, but I did need to get an access token that was long-lived, and have the ability to update this if necessary. I overcame this problem by adding some methods to my Sinatra app that will allow me to update the access token if it ever expires, or change the Facebook account used if necessary.</p>\n\n<p>The second problem I ran into wasn&#39;t really a problem, but it was a challenge to try and think of a nice way of doing it. Basically I needed to store the Facebook access token somewhere so that I could use it when I needed it - but I didn&#39;t have a database, and I didn&#39;t particularly want to add one just for storing a single string. Since this is running on Heroku (Bamboo stack, not Cedar), I was also on a read-only filesystem, so couldn&#39;t store it in a simple text file either. In this end, I chose to store the value in Memcache using the Heroku add-on. This still isn&#39;t necessarily a good solution, as this storage method isn&#39;t guaranteed to be persistent, however it should suit my needs - it doesn&#39;t particularly matter if I lose the access token, the application will gracefully degrade and just post to Twitter until I log in to Facebook via the app again.</p>\n\n<p>So, I think I&#39;ve come up with a good solution. It started off as a simple broadcaster for my blog, but I think it has a lot of potential for use with any open-source project that has social networking presence - I think it&#39;s a much more elegant and flexible hook than that which Github provides, and I hope it get&#39;s a bit of use.</p>\n"},{"title":"Howto: Database Backup and Restore","id":"howto-database-backup-and-restore","date":"2011-09-23 13:26","content":"<p>An inherent part of developing web applications is managing your datastores - typically, a relational database such as MySQL or PostgreSQL. Today, I&#39;m going to quickly cover off how to backup and restore for both of these databases.</p>\n\n<h2>What you&#39;ll need</h2>\n\n<ul>\n<li>Either PostgreSQL or MySQL</li>\n<li>Access to a database (preferably one with data in it)</li>\n</ul>\n\n<h2>Why this is useful to know</h2>\n\n<p>Lots of interactions with databases have the potential to destroy or modify data (in a bad way). When using frameworks such as Ruby on Rails, it&#39;s even easier to, say, accidently delete all of your Users (Horribly easy in DataMapper, unfortunately). It&#39;s important that before you do anything with data that&#39;s destructive, you have a backup of your database that you can restore from quickly and easily.</p>\n\n<h2>PostgreSQL</h2>\n\n<p>Postgres databases are backed up using the <code>pg_dump</code> command - a command-line utility that comes packaged with the database server. Here&#39;s the command:</p>\n\n<pre><code class=\"sh\">    pg_dump --no-owner -U [username] -W [database_name] &gt; [file to dump to].sql.dump\n</code></pre>\n\n<p>Let me explain these options and why I use them:</p>\n\n<ul>\n<li><code>--no-owner</code>: By default, Postgres dumps the database with lots of SET OWNER TO statements. I like to take these out of the dump, as I&#39;m not necessarily restoring the dump to exactly the same server with exactly the same users. Using --no-owner means that ownership statements will be excluded.</li>\n<li><code>-U [username]</code>: This lets you pass in the database user name your web application usually uses to connect - using this is just good practise, as it ensures that what you&#39;re dumping is exactly what the web application has.</li>\n<li><code>-W</code>: This option, used in conjunction with the <code>-U</code> flag, prompts for the password when you run the command</li>\n<li><code>[database_name]</code>: This is the name of the database that you want to dump</li>\n<li><code>[file to dump to]</code>: This is the file that the SQL script that pg_dump produces will be piped into. I normally name this file with the <code>.sql.dump</code> extension, so that I can see it&#39;s a SQL dump straight off.</li>\n</ul>\n\n<h4>Restoring a database dump</h4>\n\n<p>Restoring a Postgresql dump is really easy, and involves using the standard <code>psql</code> client to connect to the database and execute the SQL script in your dump file. </p>\n\n<p>First of all, make sure that you have created the database you want to load the data into. In this example, let&#39;s say I&#39;ve dumped from the <code>facebook_production</code> database to the file <code>facebook_production_23092011.sql.dump</code>, and I want to restore into the <code>facebook_development</code> database so that I can test out some code against some production data. I want to connect to the database using psql as the web application user, and load the dump in:</p>\n\n<pre><code class=\"sh\">    psql -U facebook -W facebook_development &lt; facebook_production_23092011.sql.dump`\n</code></pre>\n\n<p>Note how I am using the opposite of the less-than symbol I used above - this basically denotes the direction of the data - it&#39;s coming <em>from</em> the file, going <em>to</em> the database.</p>\n\n<p>Upon running this commmand (with your own database, of course), you will first be prompted for your database user&#39;s password, and will then see a bunch of SQL statements being executed. Once it&#39;s completed, your database has been loaded successfully - you can jump in using psql if you&#39;d like, and query around a bit.</p>\n\n<h2>MySQL</h2>\n\n<p>MySQL databases are backed up with the <code>mysqldump</code> program - one I&#39;m not as familiar with as Postgres, but I know the basics, and largely that&#39;s all you need with this type of thing. The main thing to keep in mind is that the process is the same as for PostgreSQL above - use the dump program to write the database out to a file (in the form of SQL statements), and then use the database client program <code>mysql</code> in this case, to execute the commands in the file against the database being restored to. Here&#39;s the command to dump a MySQL file to disk:</p>\n\n<pre><code class=\"sh\">    mysqldump -U [username] -P [database_name] &gt; [file to dump to].sql.dump\n</code></pre>\n\n<p>The options are more or less as I&#39;ve described above, except that <code>-P</code> is substituted for <code>-W</code>, and I&#39;ve still stuck with the <code>.sql.dump</code> naming scheme.</p>\n\n<h4>Restoring a database dump</h4>\n\n<p>This process is almost identical to the PostgreSQL restore process. Let&#39;s stick with the same example format we already have - dumping from a database called <code>facebook_production</code> to file <code>facebook_production_23092011.sql.dump</code>, restoring into a database called <code>facebook_development</code> - here&#39;s the command we need for that:</p>\n\n<pre><code class=\"sh\">    mysql -U facebook -P facebook_development &lt; facebook_production_23092011.sql.dump\n</code></pre>\n\n<p>Once again, you&#39;ll be prompted for your password, but for this one you won&#39;t see the output of the SQL statements - it will take a couple of seconds, and then the program will exit. This is normal however - if you&#39;d like to see the output of the batch load, you can add <code>-v -v -v</code> before the <code>&lt;</code> symbol to turn verbosity to level three, otherwise you can go ahead and jump into your database and make sure everything is there.</p>\n\n<h2>Tips:</h2>\n\n<ul>\n<li>If you don&#39;t have a database user account yet, on Unix and Linux you can use <code>sudo su postgres</code> to login as the &#39;postgres&#39; user - a root-like database user that gets created for you when Postgres is installed. MySQL has a root account, but it&#39;s not a system user - you are normally prompted for a root username and password when you install MySQL. If you <em>are</em> using the Postgres user, you don&#39;t need to pass in a username or password in the above commands, but you need to remember that your restored databases will be owned by &#39;postgres&#39;, not you web application database user.</li>\n<li>A handy place to put database dumps that you are planning to use right away is <code>/tmp</code> (Only on Unix and Linux). This is a directly that is writeable by everyone, that will get &#39;garbage collected&#39; periodically. This is especially handy if you are logging in as the &#39;postgres&#39; user, as typically this user won&#39;t have write permissions on many other directories</li>\n</ul>\n"},{"title":"Overriding Action Caches","id":"overriding-action-caches","date":"2011-09-30 11:48","content":"<p>Recently, I have been working on a web application that is quite media rich, and is expected to run into quite a bit of traffic. I&#39;ve been working on building an API for a front end system, using JSON to handle passing this data back and forth.</p>\n\n<p>Obviously with this amount of traffic, and the size of some of the JSON collections we were having to marshall and store via Rails, we were going to need some pretty intense caching to reduce the load on Rails, and our database server. We had to balance this need, however, with the requirement that data should be live, or close to live (i.e. around 5-10 minutes) - in particular, statistics, which are calculated on-demand for several responses.</p>\n\n<p>The strategy we chose to manage this balance was to use Rails&#39; provided <code>caches_action</code> method to cache our JSON responses, building up a cache key from certain parameters, as well as some meta-data, for example, the user&#39;s logged-in status. Because we were using memcached, we could use the <code>:expires_in</code> option to tell the memcached store to expire the cached value after x minutes.</p>\n\n<p>This approach worked for a while, but we found we had a pretty major problem - while the data was cached, it went alright, but as soon as the cache expired we were having loads of users hitting a response that took way to long to build (before we optimized queries, 30+ seconds). So, we needed another fix.</p>\n\n<p>To fix this problem, we tried out adding some cron tasks that used curl to ping the cached URLs, to try and preload the cache so that less users would be hitting the DB. This only partially fixed the problem though, so we identified a solution that would work a little better for us.</p>\n\n<p>What we wanted to do was to leave our existing caching in place - aside from the expiry, it was working fine, and we didn&#39;t want to rework everything. With this in mind though, we needed a way to force a refresh of the data in the cache external from the controller. What we ended up implementing was a monkeypatch on Rails&#39; caches<em>action-related methods, that allows us to pass in an <code>:overwrite</code> option - this can be a Proc, or just a boolean - basically, when the value of <code>:overwrite</code> is true, Rails will bypass the cached value, grab the _new value</em>, and load this into the cache - effectively refreshing the value without a user having to trigger the process. </p>\n\n<p>Here&#39;s the monkeypatch code - have a scan through it, and I&#39;ll explain it below:</p>\n\n<pre><code class=\"ruby\">    require &#39;set&#39;\n\n    module ActionController #:nodoc:\n      module Caching\n\n        module Actions\n        extend ActiveSupport::Concern\n\n          protected\n          class ActionCacheFilter #:nodoc:\n            def initialize(options, &amp;block)\n              @cache_path, @store_options, @cache_layout =\n                options.values_at(:cache_path, :store_options, :layout)\n            end\n\n            def filter(controller)\n              path_options = if @cache_path.respond_to?(:call)\n                controller.instance_exec(controller, &amp;@cache_path)\n              else\n                @cache_path\n              end\n\n              cache_path = ActionCachePath.new(controller, path_options || {})\n              overwrite = if @overwrite = @store_options.fetch(:overwrite, nil)\n                @overwrite.respond_to?(:call) ? controller.instance_exec(controller, &amp;@overwrite) : @overwrite\n              else\n                false\n              end\n\n              body = overwrite ? nil : controller.read_fragment(cache_path.path, @store_options)\n\n              unless body\n                controller.action_has_layout = false unless @cache_layout\n                yield\n                controller.action_has_layout = true\n                body = controller._save_fragment(cache_path.path, @store_options)\n              end\n\n              body = controller.render_to_string(:text =&gt; body, :layout =&gt; true) unless @cache_layout\n              controller.response_body = body\n              controller.content_type = Mime[cache_path.extension || :html]\n            end\n          end\n        end\n      end\n    end\n</code></pre>\n\n<p>By dropping this code into <code>config/initializers</code>, this code gets patched into the ActionController::Caching::Actions::ActionCacheFilter class, and overrides the <code>initalize</code> and <code>filter</code> methods to let us a) pass in an override option, and b) choose to refresh the cache if the override option is set.</p>\n\n<p>The filter method performs as normal until it has finished generating the cache path - at this point, it would normally return the cached response if it was there, and if it had not expired. Instead, my colleague <a href=\"http://telos.co.nz\">James Moriaty</a> replaced some code here:</p>\n\n<ul>\n<li>First, it retrieves the value of the <code>:overwrite</code> option passed in to the <code>caches_action</code> method from the <code>@store_options</code> hash - if it&#39;s a Proc, it executes it here to get the value, otherwise assumes it&#39;s a boolean variable.</li>\n<li>If the <code>:overwrite</code> option has not been passed in, it returns false - i.e. don&#39;t overwrite the cache.</li>\n<li>If the overwrite value is true, it sets the body to nil, so that it will be re-built. Otherwise, it does the usual and returns the cached response from Memcache.</li>\n<li>From here, it more or less goes back to the default class, rebuilding the response from the database.</li>\n</ul>\n\n<p>In our application&#39;s case, we use this functionality by tweaking our cron jobs a little to pass in a particular parameter - we then added the <code>:overwrite</code> option to our <code>caches_action</code> methods, with a Proc that returns true if this parameter equals the correct value. </p>\n\n<p>So far, this solution has worked fantastically - now, hardly any of our users hit the database - instead, they are heading to memcache to grab that response, while our background cron jobs rebuild the data that will get returned to them. Using a parameter for refreshing the cache also lets us easily refresh manually for testing or to check for a value. </p>\n\n<p>This solution is clean, simple and easy to implement. I suggest that if you are facing similar problems, that you give it a go - it&#39;s really adaptable, and requires few changes if you are already using action caching. Full credit to James for thinking up and implementing this solution - I&#39;m just documenting it.</p>\n"},{"title":"Action Mailer Interceptors","id":"action-mailer-interceptors","date":"2011-10-06 16:51","content":"<p>ActionMailer Interceptors are a great way to test the full stack of your mailing in Rails from the generation from data through to receiving the email in your client. They are similar to ActiveRecord&#39;s <code>before_x</code>-type callbacks, and let you change something about the message being sent right before it&#39;s actually dispatched.</p>\n\n<p>The most common purpose I use these for is to redirect mail being sent from the application to either my Inbox, or some shared account (if it&#39;s on a project where I&#39;m not the only developer). This lets me make sure that the HTML in the message is displayed properly, and that the data gets injected into the content the way I expect it to be.</p>\n\n<p>Here&#39;s a quick example of how to use a Mail interceptor for this purpose:</p>\n\n<pre><code class=\"ruby\">    # lib/development_mail_interceptor.rb\n    class DevelopmentMailInterceptor\n      def self.delivering_email(mail)\n        mail.subject = &quot;&lt;#{mail.to}&gt; #{mail.subject}&quot;\n        mail.to = &quot;me@mywork.co.nz&quot;\n      end\n    end\n\n    # config/initializers/mail_interceptors.rb\n    require &#39;development_mail_interceptor&#39;\n    ActionMailer::Base.register_interceptor(DevelopmentMailInterceptor) if Rails.env.development?\n</code></pre>\n\n<p>...and give one of your mailers a go.</p>\n\n<p>This is a very simple example, of course, but there are a lot of uses for this type of thing - logging &amp; auditing, checking against quotas, analysis, etc. etc. </p>\n\n<p>What really triggered this post though, was a odd problem I came across when trying to get this interceptor to work. An ActionMailer method can be triggered using one of two methods: either <code>deliver</code> or <code>deliver!</code> - the main difference between the two is that the second will throw exceptions if it cannot be sent, which is why I tend to prefer using it. Something to keep in mind though, is that using <code>deliver!</code> will call any registered Mail Observers, but <strong>not Interceptors</strong> - meaning that your mail will be sent unaltered.</p>\n\n<p>It really was a frustrating process to debug, but after looking at the <a href=\"https://github.com/mikel/mail\">Mail gem</a> source code (ActionMailer back-ends onto this gem for it&#39;s mail setup and delivery processes), in particular the <a href=\"https://github.com/mikel/mail/blob/master/lib/mail/message.rb#L227\">Message class</a>, I noticed this crucial difference between the two. Resolving the issue is, of course, as simple as using <code>deliver</code> - without the exclamation mark. After that, your Interceptors will be triggered just as they should be. </p>\n"},{"title":"Safely Start and Stop VirtualBox VMs with init.d","id":"safely-start-and-stop-virtualbox-vms-with-initd","date":"2011-10-19 17:05","content":"<p>Recently I&#39;ve rolled out a virtual machine host box to run headless VMs (headless means that there is no display, keyboard etc plugged into it), as these make great test and experiment machines for trying new things out. As part of this rollout, I mashed a couple of blogs and some documentation together and came up with this script:</p>\n\n<p>{% codeblock /etc/init.d/virtual_machines lang:bash %}</p>\n\n<h1>!/bin/sh</h1>\n\n<h1>virtual_machines  Start and stop virtual machines when the host changes state</h1>\n\n<p>VMUSER=administrator</p>\n\n<p>case &quot;$1&quot; in\n    start)\n        echo &quot;Starting VirtualBox VMs&quot;\n        if [ -f /etc/virtualbox/machines<em>enabled ]; then\n            cat /etc/virtualbox/machines</em>enabled | while read VM; do\n              sudo -H -b -u $VMUSER /usr/bin/VBoxHeadless -startvm &quot;$VM&quot;\n            done\n        fi\n        ;;\n    stop)\n        echo &quot;Saving state of VirtualBox VM...&quot;\n        cat /etc/virtualbox/machines<em>enabled | while read VM; do\n          sudo -H -u $VMUSER /usr/bin/VBoxManage controlvm &quot;$VM&quot; savestate\n        done\n        ;;\n    *)\n        echo &quot;Usage: /etc/init.d/virtual</em>machines {start|stop}&quot;\n        exit 1\n        ;;\nesac</p>\n\n<p>exit 0</p>\n\n<p>{% endcodeblock %}</p>\n\n<p>I wrote this because this isn&#39;t something that is supported directly by VirtualBox, and it&#39;s essential that on a headless server that these virtual machines be able to go up and down happily without harm and reliably. It supports start and stop explanations and uses savestate, rather than poweroff - essentially, as the host server goes down, it will &#39;hibernate&#39; each VM, and then restore when the server starts back up again.</p>\n\n<p>Another feature that I built into this was the use of a file in which a list of VMs can be specified to start and stop - for example, if you only want this script to deal with a subset of the virtual machines you have set up on the host server.</p>\n\n<p>An example of this file might look like this:</p>\n\n<p>{% codeblock /etc/virtualbox/virtual<em>machines %}\nwinxp</em>development\nwin7<em>development\nubuntu</em>11<em>10</em>development\n{% endcodeblock %}</p>\n\n<p>It works really well for me, and since it&#39;s something I had trouble tracking down, hopefully I can make someone&#39;s life a little easier with this solution.</p>\n"},{"title":"A Fun Little Bookmarklet","id":"a-fun-little-bookmarklet","date":"2011-10-20 13:28","content":"<p>I&#39;m sure this has been done before, but after noticing <a href=\"http://www.1-day.co.nz\">1-day&#39;s</a>  &#39;Look Busy&#39; feature, I just had to write a bookmarklet to load this up on any site!</p>\n\n<p>If you just want to try it out, here&#39;s the link <a href=\"javascript:var busy=document.createElement('div');var body=document.getElementsByTagName('body')[0];var max_width_cache=body.getAttribute('max-width');body.style.maxWidth='100%';busy.setAttribute('id','lookbusy');busy.setAttribute('style','position: absolute; z-index: 1000; width: 100%; height: 100%; top: 0px; left: 0px; right: 0px; bottom: 0px; background: #FFF url(http://www.1-day.co.nz/images/2010_mission_critical_development_strategy.png) no-repeat 0 0;');var close=document.createElement('a');close.setAttribute('class','busy close');close.setAttribute('style','position: fixed; z-index: 1001; right: 10px; bottom: 10px; background-color: #FFF; color: #000; font-size:10px');close.setAttribute('href','#');close.innerText='OK, Clear';close.setAttribute('onclick',&quot;javascript:body.removeChild(document.getElementById('lookbusy'));body.style.maxWidth = &quot; + max_width_cache);busy.appendChild(close);body.appendChild(busy)\">Look Busy Bookmarklet</a> - either right-click on the link and &#39;Copy URL&#39; and paste into a new bookmark, or drag to your bookmarks bar or folder. It should work in Chrome, Safari, Firefox and IE7-9</p>\n\n<p>Here&#39;s the source code for anyone interested in seeing how this works:</p>\n\n<p>{% codeblock Look Busy Bookmarklet lang:javascript %}\n// Create a new div tag to contain our image\nvar busy = document.createElement(&#39;div&#39;);\nvar body = document.getElementsByTagName(&#39;body&#39;)[0];\nvar max<em>width</em>cache = body.style.maxWidth;\nbody.style.maxWidth = &#39;100%&#39;;\nbusy.setAttribute(&#39;id&#39;, &#39;lookbusy&#39;);</p>\n\n<p>// Add a background image, and position the div tag above all other content and make it fill the screen\nbusy.setAttribute(&#39;style&#39;, &#39;position: absolute; z-index: 1000; width: 100%; height: 100%; top: 0px; left: 0px; right: 0px; bottom: 0px; background: #FFF url(<a href=\"http://www.1-day.co.nz/images/2010_mission_critical_development_strategy.png\">http://www.1-day.co.nz/images/2010_mission_critical_development_strategy.png</a>) no-repeat 0 0;&#39;);</p>\n\n<p>// Add a inconspicuous link to close the image\nvar close = document.createElement(&#39;a&#39;);\nclose.setAttribute(&#39;class&#39;, &#39;busy close&#39;);\nclose.setAttribute(&#39;style&#39;, &#39;position: fixed; z-index: 1001; right: 10px; bottom: 10px; background-color: #FFF; color: #000; font-size:10px&#39;);\nclose.setAttribute(&#39;href&#39;, &#39;#&#39;);\nclose.innerText = &quot;OK, Clear&quot;;\nclose.setAttribute(&#39;onclick&#39;, &quot;javascript:body.removeChild(document.getElementById(&#39;lookbusy&#39;));body.style.maxWidth = &quot; + max<em>width</em>cache);</p>\n\n<p>// Add the close image to the look busy div tag\nbusy.appendChild(close);</p>\n\n<p>// Add the look busy div tag to the body of the document\nbody.appendChild(busy);</p>\n\n<p>{% endcodeblock %}</p>\n\n<p>Give it a go! It&#39;s super handy for when you&#39;re checking out <a href=\"http://www.trademe.co.nz\">TradeMe</a>, <a href=\"http://failblog.org\">Failblog</a>, or <a href=\"http://images.google.com?q=wink\">anything similar</a>!</p>\n"},{"title":"Generating 'Gem Install' Commands From 'Gem List'","id":"generating-gem-install-commands-from-gem-list","date":"2011-11-03 16:34","content":"<p>Here at <a href=\"http://3months.com\">3Months</a>, we have a couple of monolithic projects that have been around for yonks - because of this, they don&#39;t have bundler set up, and many of them have incomplete or out-of-date gem requirements. Yesterday I needed to get one of these projects set up locally for the first time, so I was kindly lent the output of <code>gem list</code> with which to install the gems at the correct versions required. </p>\n\n<p>To make this problem a little easier in the future, I wrote a quick script to generate a massive shell command to install all the gems recorded in <code>gem list</code>. Next time I, or someone else, needs to set up one of these types of project, I can run this script, and hand them the generated shell script. They can run this in their gemset of choice, and install all gems required - much easier!</p>\n\n<p>Here&#39;s the script - alternatively, you can check it out the <a href=\"https://gist.github.com/1335721\">Gist</a>:</p>\n\n<p>{% codeblock lang:ruby %}</p>\n\n<h1>!/usr/bin/env ruby</h1>\n\n<h3>gem<em>to</em>command.rb</h3>\n\n<h1>Produce a command to install the gems you currently have installed (using gem list)</h1>\n\n<h1>Makes a simple Shell script that can be run on Linux or Mac OS X</h1>\n\n<h1>Author: @sudojosh</h1>\n\n<p>gems = <code>gem list</code>\nFile.open(&quot;install_gems.sh&quot;, &quot;wb&quot;) do |script|\n  script &lt;&lt; &quot;#!/usr/bin/env ruby\\n&quot;\n  gems = gems.split(&quot;\\n&quot;).map { |gem| \n    gem = gem.split(&#39; &#39;).map { |part| \n      part.gsub(/[^\\w.-]/, &#39;&#39;) \n    }; \n    [gem[0], gem[1..-1].min] </p>\n\n<p>}\n  gems.each { |gem|\n    script &lt;&lt; &quot;gem install #{gem[0]} -v=#{gem[1]}&quot;\n    script &lt;&lt; &quot; &amp;&amp; &quot; unless gem[0] == gems.last[0]\n  }\nend\nputs &quot;Wrote install_gems.sh&quot;\n{% endcodeblock %}</p>\n\n<p>Here&#39;s what it does:</p>\n\n<ul>\n<li>Get the output of <code>gem list</code></li>\n<li>Open a .sh file for writing</li>\n<li>Iterate through the <code>gem list</code> output, and parse from the file the name of the gem and the lowest version number (lowest is safer than highest, even though there <em>should</em> only ever be one)</li>\n<li>Assemble a <code>gem install</code> command with the <code>--no-rdoc --no-ri</code> arguments, and write this to the file.</li>\n</ul>\n\n<p>Enjoy!</p>\n"},{"title":"Add jQuery to Any Page Really, Really Easily","id":"add-jquery-to-any-page-really-really-easily","date":"2011-11-12 21:27","content":"<p>When I&#39;m working on a site, or analysing somebody else&#39;s, I often wish that jQuery was loaded into that page - it&#39;s an unbeatable tool for really digging into the site&#39;s source to debug something or work out how something has been done.</p>\n\n<p>To help out with this, I&#39;ve put together a quick bookmarket, <a href=\"http://blog.joshmcarthur.com/2011/10/20/a-fun-little-bookmarklet/\">similar to the more fun one I&#39;ve done a while ago</a>. It works extremely simply - it just creates a script element, adds jQuery 1.7.0 (served from Google), and appends it to the page body. As soon as the script has loaded, jQuery can be used from within Web Inspector, Firebug, or whatever other Javascript console you use.</p>\n\n<p>Here&#39;s the bookmarklet:</p>\n\n<p><a href=\"javascript:var s=document.createElement('script');s.type='text/javascript';s.src='https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js';document.getElementsByTagName('body')[0].appendChild(s);\">Add jQuery</a></p>\n\n<p>To add this to your browser, you can either: right-click on this link, select &#39;Bookmark Link&#39;, or just drag-and-drop onto your Bookmarks toolbar. Once it&#39;s added, you can simply click on the bookmark on any page, and jQuery will be loaded into the page for you to use. </p>\n\n<p>Here&#39;s the source:</p>\n\n<p>{% codeblock Add jQuery Bookmarklet lang:javascript %}\n// Create the script tag\nvar s = document.createElement(&#39;script&#39;);</p>\n\n<p>// Set the necessary attributes on the tag\ns.type = &quot;text/javascript&quot;;\ns.src = &quot;<a href=\"https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js\">https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js</a>&quot;;</p>\n\n<p>// Add to the body tag (assume here we&#39;re dealing with HTML, and there IS a body tag)\ndocument.getElementsByTagName(&#39;body&#39;)[0].appendChild(s);\n{% endcodeblock %}</p>\n"},{"title":"Upcoming: RateMyCourses","id":"upcoming-ratemycourses","date":"2011-11-14 20:16","content":"<p>Tonight I&#39;d like to talk about a project I&#39;ve been working on in my spare time for the\nlast few months. I&#39;m super excited to see it coming together, and it&#39;s <em>nearly</em> ready to\ngo live.</p>\n\n<p>The application is called <strong>RateMyCourses</strong> - put simply, it&#39;s a way for University students\nto explore, rate, and comment on courses they&#39;ve done to help out others. Inspired by\nsuch projects as <a href=\"http://www.fixmystreet.com\">Fix my Street</a> and <a href=\"http://www.fixmytransport.com\">Fix my Transport</a>, <strong>RateMyCourses</strong> aims to increase the transparency of quality courses, lecturers, and tertiary education institutes within New Zealand. Over the last few months I&#39;ve been not only building up this application, but also generating the dataset of courses thats at the heart of the application from scratch, pulling course details from all major Universities within New Zealand, including:</p>\n\n<ul>\n<li>Auckland University</li>\n<li>Massey University</li>\n<li>Waikato University</li>\n<li>Victoria University of Wellington</li>\n<li>University of Canterbury</li>\n<li>Lincoln University</li>\n<li>Otago University</li>\n</ul>\n\n<p>All up, I&#39;ve got nearly 20,000 courses indexed, from these seven institutions. This is a huge opportunity for students to really engage with each other, with their Universities, and with new entrants. Over time, I&#39;m hoping <strong>RateMyCourses</strong> will become a valuable tool for course planning, as it uncovers the best courses, institutions, and educators in New Zealand (and, eventually, other countries).</p>\n\n<p>This is just a preliminary post while I polish up some areas, and make sure I&#39;m really happy with things, but I welcome feedback and comments as I move into the launch and adoption phases. I&#39;m planning to launch in the upcoming weeks, and spend the summer with a core group of users ironing out the major issues, before really pushing it at Victoria next year.</p>\n"},{"title":"Treatme Lite: Adventures in Javascript","id":"treatme-lite-adventures-in-javascript","date":"2012-02-06 21:01","content":"<p>For a long time, I&#39;ve been looking to move into the <a href=\"http://nodejs.org\">NodeJS</a> trend that&#39;s been taking the development community by storm. The problem is that coming from a Rails development background, and my inability to follow callbacks through more than 2 levels, has lead to my previous efforts to end in broken code and frustration.</p>\n\n<p>Happily, however, I took another look at <a href=\"http://expressjs.com\">express</a>, a Sinatra-like framework for NodeJS, and it clicked with me on a level that vanilla Node had not in the past. It supported middleware, was reasonably well-documented, and I was able to find <a href=\"https://github.com/twilson63/express-coffee\">a handy project on Github</a> which allowed me to start from a solid base of proven middleware and NodeJS modules to help me out (I typically now develop in <a href=\"http://coffeescript.org\">Coffeescript</a> for my personal projects - I enjoy the ability to code in something that takes my missing knowledge of how to write beautiful Javascript into account).</p>\n\n<p>Next, I needed something to build - ideally something I could become reasonably passionate about finishing, but that also would not consume more than a week or so of working in the evenings. Something I&#39;ve been using recently is <a href=\"http://treatme.co.nz/now\">TreatMe Now</a> - a sort of customers-on-demand voucher system for retailers that is updated throughout the day with some really good deals - typically, these last only a couple of hours though, so it&#39;s important to check quite often. Something else I had lamented is that, while there is an iPhone app available, there was no mobile web support, no support for other devices, and the normal web interface is chock full of data-heavy maps and unfriendly panels that are, frankly, downright annoying to use on a smartphone.</p>\n\n<p>Using Chrome&#39;s web inspector, I was able to find a JSON feed of deals, and work out how to filter the results to a particular region. Given this datasource, I was ready to begin building out this experiment, using a number of resources I&#39;ve wanted to try out in more detail - NodeJS, Express, more in-depth HTML5 things, responsive design, and an adaptive grid system (Normally I use <a href=\"https://twitter.github.com/bootstrap\">Twitter Bootstrap</a>, but I made a conscious effort to keep things light, using Skeleton&#39;s adaptive grid system, and Zepto.js in place of jQuery). This blog post is going to detail how I went about building backend and fronted of the application, to the point where I&#39;ve got <a href=\"http://treatmelite.herokuapp.com\">The Final Product</a>, an express-backed single-page application supporting offline access, local storage and a fully responsive design.</p>\n\n<p>Over the next three posts, I&#39;m going to be documenting how I went about building a responsive and HTML5-based web application to serve TreatMe Deals, using NodeJS, Express, ZeptoJS,  Coffeescript and the Skeleton CSS framework, including difficulties I ran into, and how I solved them:</p>\n\n<ul>\n<li>Part One: The Backend - an examination of my basic web service written in <a href=\"http://expressjs.org\">Express</a> which serves a JSON datasource of deals, and makes calls to Google&#39;s Geocoding service</li>\n<li>Part Two: The Frontend - a look at the Javascript and CSS tools I used to develop the frontend of the application</li>\n<li>Part Three: Responsiveness and Extras - some detail about how I added support for offline asset caching and local storage of deals, and made additional tweaks to make the application work fully offline.</li>\n</ul>\n"},{"title":"Treatme Lite: Part One - the Backend","id":"treatme-lite-part-one---the-backend","date":"2012-02-08 21:07","content":"<h2>The Backend</h2>\n\n<blockquote>\n<p>This article first is in a series of three blog posts documenting how I created <a href=\"http://treatmelite.herokuapp.com\">TreatMe Lite</a>, an HTMl5 web app using Zepto.js, Coffeescript, and a range of other frameworks and tools, all backed by an <a href=\"http://expressjs.org\">Express JS</a> web service. See my <a href=\"/2012/02/06/treatme-lite-adventures-in-javascript/\">previous post</a> for more information about the application.</p>\n</blockquote>\n\n<p>Last post (the intro), I left off explaining why I set out the build the application, and the technologies I had selected to build each part. This post is going to detail the steps I took to create the web service using <a href=\"http://nodejs.org\">NodeJS</a> and <a href=\"http://expressjs.org\">Express</a>.</p>\n\n<p>First of all, I needed a way to serve the deal JSON. In actuality, this is something that I could have done just as easily directly from the front-end Javascript, however, I&#39;d started off with a NodeJS application, and it seemed just as sensible to request the JSON from <a href=\"http://treatme.co.nz\">TreatMe</a> using Node&#39;s <a href=\"http://nodejs.org/docs/latest/api/http.html#http.get\">support for HTTP requests</a>.</p>\n\n<p>To kick off the node app, I cloned a starter application from Github:</p>\n\n<pre><code>git clone git://github.com/twilson63/express-coffee.git treatmelite\n</code></pre>\n\n<p>This application gives us an easy base with support for <a href=\"http://coffeescript.org\">Coffeescript</a>, <a href=\"http://jade-lang.com/\">Jade templating</a> and a bunch of other CSS stuff, like the <a href=\"http://getskeleton.com\">Skeleton grid</a> (which I&#39;m a big fan of).</p>\n\n<p>To satisfy the requirements for the front-end, my NodeJS application needed to perform two tasks:</p>\n\n<h4>/deals/:latitude/:longitude: Retrieve a list of nearby deals from TreatMe</h4>\n\n<p>The code:</p>\n\n<p>{% gist 1766753 app.coffee %}</p>\n\n<p>Explanation:</p>\n\n<ul>\n<li>First, we make the HTTP request, passing a callback that will get a response object as an argument</li>\n<li>We listen for certain events while we receive the data from the response - if we hit an error, we return an error message. We listen for when we receive some data, and append it to the string we want to return to the front-end, and when we reach the end of the data, we return this string.</li>\n</ul>\n\n<h4>/geocode: Geocode an address, and pass back the information we need to display to the user</h4>\n\n<p>The Code:</p>\n\n<p>{% gist 1766776 app.coffee %}</p>\n\n<p>Explanation:</p>\n\n<ul>\n<li>We build the path to the Google Geocoder using either the latitude and longitude from the user&#39;s current location (i.e. reverse geocoding), or the address (typically a city name like &#39;Wellington, New Zealand&#39;)</li>\n<li>We follow a similar pattern to the Deals action, requesting a geocoding result, waiting for a response, and parsing the data we need out of that response. In this case, we build a JSON structure containing the geocoded address, it&#39;s latitude, and it&#39;s longitude.</li>\n</ul>\n\n<blockquote>\n<p>A note on Geocoding: We use <a href=\"http://%20code.google.com/apis/maps/documentation/geocoding/\">Google&#39;s Geocoding service</a> to retrieve the information we need from either the name of a location, or the latitude and longitude (either way, it fills in the information we need). Once again, this is something we ideally would do from the front-end, however, I ran into some cross domain issues, and so simplified matters by performing the request on the server</p>\n</blockquote>\n"},{"title":"Git-browse for Quick Repo Viewing","id":"git-browse-for-quick-repo-viewing","date":"2012-02-28 08:50","content":"<p>Last night I quickly patched together a command called <code>git-browse</code> - it&#39;s a small but handy extension to <a href=\"http://git-scm.com/\">git</a> that looks at the remotes you have set up inside a repository, and opens up the first Github repository it finds - to give an example:</p>\n\n<p><pre>\n-&gt; git remote -v\norigin <a href=\"mailto:git@github.com\">git@github.com</a>:joshmcarthur/WriteIdeally.git (fetch)\norigin <a href=\"mailto:git@github.com\">git@github.com</a>:joshmcarthur/WriteIdeally.git (push)\nheroku <a href=\"mailto:git@heroku.com\">git@heroku.com</a>:writeideally.git (fetch)\nheroku <a href=\"mailto:git@heroku.com\">git@heroku.com</a>:writeideally.git (push)\n-&gt; git browse\n(Opens <a href=\"https://github.com/joshmcarthur/WriteIdeally\">https://github.com/joshmcarthur/WriteIdeally</a> in default browser)\n</pre>\nl\nThe source is just below - at the moment, though it&#39;s only compatible with OS X and Linux - Ubuntu that I know of, but I believe that RedHat won&#39;t work without installing packages - see <a href=\"http://stackoverflow.com/questions/5116473/linux-command-to-open-url-in-default-browser\">this StackOverflow thread</a></p>\n\n<p>{% gist 1926673 git-browse %}</p>\n\n<p>Installation is pretty basic - just download, make it executable by running <code>chmod +x git-browse</code>, and then copy it somewhere that is referenced by your <code>$PATH</code> variable (you can see what&#39;s in this variable by running <code>echo $PATH</code> in a Terminal). Once you&#39;ve done this, you should be able to enter any repository with Github remotes, and run <code>git browse</code> to have the repository open in your default browser.</p>\n\n<p>Extensions to this idea should be pretty easy - something I would be keen to see in a pull request, for example, would be to support opening arbitrary files - i.e. <code>git browse lib/writeideally/api.rb</code> would open <a href=\"https://github.com/joshmcarthur/WriteIdeally/blob/master/lib/writeideally/api.rb\">https://github.com/joshmcarthur/WriteIdeally/blob/master/lib/writeideally/api.rb</a>. Having said that, my gsubbing is pretty gross, so that needs refactoring as well.</p>\n\n<p>In any case, enjoy!</p>\n"}]